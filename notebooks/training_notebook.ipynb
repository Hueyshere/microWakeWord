{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a yaml config that controls the training process\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "config = {}\n",
    "\n",
    "config[\"train_dir\"] = \"trained_models/alexa_model\"\n",
    "\n",
    "# Each [\"features\"] entry has the following parameters that dictate how the spectrograms are weighted, whether they represent the wake word or not, and how they are truncated\n",
    "#  sampling_weight: Weight for choosing a spectrogram from this set in the batch\n",
    "#  penalty_weight: Penalizing weight for incorrect predictions from this set\n",
    "#  truth: Boolean whether this set has positive samples or negative samples\n",
    "#  truncation_strategy = If spectrograms in the set are longer than necessary for training, how are they truncated\n",
    "#       - random: choose a random portion of the entire spectrogram - useful for long negative samples\n",
    "#       - truncate_start: remove the start of the spectrogram\n",
    "#       - truncate_end: remove the end of the spectrogram\n",
    "#       - split: Split the longer spectrogram into separate spectrograms offset by 100 ms. Only for ambient sets\n",
    "config[\"features\"] = [\n",
    "    # You can augment clips 'on-the-fly' while training\n",
    "    # The dictionary entries correspond to arguments for the Clips, Augmentation, and SpectrogramGeneration classes\n",
    "    {\n",
    "        'clips_settings': {\n",
    "            'input_directory': 'generated_samples/positive/training',\n",
    "            'file_pattern': '*.wav',\n",
    "            'max_clip_duration_s': 1.29,   \n",
    "            'min_clip_duration_s': 0.5,\n",
    "        },\n",
    "        'augmentation_settings': {\n",
    "            \"impulse_paths\": [\n",
    "                \"./mit_rirs\"\n",
    "            ],\n",
    "            \"background_paths\": [\n",
    "                \"./audioset_16k\", \"./fma\"\n",
    "            ],\n",
    "            'augmentation_probabilities': {\n",
    "                    \"SevenBandParametricEQ\": 0.0,\n",
    "                    \"TanhDistortion\": 0.0,\n",
    "                    \"PitchShift\": 0.0,\n",
    "                    \"BandStopFilter\": 0.0,\n",
    "                    \"AddColorNoise\": 0.25,\n",
    "                    \"AddBackgroundNoise\": 1.0,\n",
    "                    \"Gain\": 1.0,\n",
    "                    \"RIR\": 0.25,\n",
    "            },\n",
    "            'augmentation_duration_s': 3.99,\n",
    "            'max_jitter_s': 0.2,\n",
    "            'min_jitter_s': 0.1,\n",
    "            'background_min_snr_db': -10,\n",
    "            'background_max_snr_db': 5,\n",
    "        },\n",
    "        'spectrogram_generation_settings': {\n",
    "            \"slide_frames\": 5,\n",
    "        },\n",
    "        'truncation_strategy': 'truncate_start',\n",
    "        'sampling_weight': 0.5,\n",
    "        'penalty_weight': 1,\n",
    "        'truth': True,\n",
    "        'type': \"clips\",   \n",
    "    },\n",
    "    {\n",
    "        'clips_settings': {\n",
    "            'input_directory': 'generated_samples/negative/training',\n",
    "            'file_pattern': '*.wav',\n",
    "            'max_clip_duration_s': 3.69,   \n",
    "            'min_clip_duration_s': None,\n",
    "        },\n",
    "        'augmentation_settings': {\n",
    "            \"impulse_paths\": [\n",
    "                \"./mit_rirs\"\n",
    "            ],\n",
    "            \"background_paths\": [\n",
    "                \"./audioset_16k\", \"./fma\"\n",
    "            ],\n",
    "            'augmentation_probabilities': {\n",
    "                    \"SevenBandParametricEQ\": 0.0,\n",
    "                    \"TanhDistortion\": 0.0,\n",
    "                    \"PitchShift\": 0.0,\n",
    "                    \"BandStopFilter\": 0.0,\n",
    "                    \"AddColorNoise\": 0.25,\n",
    "                    \"AddBackgroundNoise\": 0.9,\n",
    "                    \"Gain\": 1.0,\n",
    "                    \"RIR\": 0.33,\n",
    "            },\n",
    "            'augmentation_duration_s': 3.99,\n",
    "            'max_jitter_s': 0.2,\n",
    "            'min_jitter_s': 0.1,\n",
    "            'background_min_snr_db': -10,\n",
    "            'background_max_snr_db': 0,\n",
    "        },\n",
    "        'spectrogram_generation_settings': {\n",
    "            \"slide_frames\": 5,\n",
    "        },\n",
    "        'truncation_strategy': 'truncate_start',\n",
    "        'sampling_weight': 0.5,\n",
    "        'penalty_weight': 0.33,\n",
    "        'truth': False,\n",
    "        'type': \"clips\",   \n",
    "    },\n",
    "# Each features_dir should have at least one of the following folders with this structure when you are using the \"mmap\" type:\n",
    "#  training/\n",
    "#    ragged_mmap_folders_ending_in_mmap\n",
    "#  testing/\n",
    "#    ragged_mmap_folders_ending_in_mmap\n",
    "#  testing_ambient/\n",
    "#    ragged_mmap_folders_ending_in_mmap\n",
    "#  validation/\n",
    "#    ragged_mmap_folders_ending_in_mmap\n",
    "#  validation_ambient/\n",
    "#    ragged_mmap_folders_ending_in_mmap\n",
    "#\n",
    "#  You need at least one of the 5 root folders\n",
    "    {\n",
    "        \"features_dir\": \"generated_sets\",\n",
    "        \"sampling_weight\": 0.0,\n",
    "        \"penalty_weight\": 1,\n",
    "        \"truth\": True,\n",
    "        \"truncation_strategy\": \"truncate_start\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"negative_datasets/speech_background\",\n",
    "        \"sampling_weight\": 5.0,\n",
    "        \"penalty_weight\": 1,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"negative_datasets/dinner_party_background\",\n",
    "        \"sampling_weight\": 5.0,\n",
    "        \"penalty_weight\": 1,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"negative_datasets/no_speech_background\",\n",
    "        \"sampling_weight\": 2,\n",
    "        \"penalty_weight\": 1,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Number of training steps in each iteration - various other settings are configured as lists that corresponds to different steps\n",
    "config[\"training_steps\"] = [20000, 20000, 20000, 20000]\n",
    "\n",
    "# Penalizing weight for incorrect class predictions - lists that correspond to training steps\n",
    "config[\"positive_class_weight\"] = [1]\n",
    "config[\"negative_class_weight\"] = [1, 1.5, 2, 3]\n",
    "\n",
    "# Learning rates for Adam optimizer - list that corresponds to training steps\n",
    "config[\"learning_rates\"] = [\n",
    "    0.001,\n",
    "    0.0005,\n",
    "    0.0002,\n",
    "    0.0001,\n",
    "] \n",
    "config[\"batch_size\"] = 128\n",
    "\n",
    "#SpecAugment parameters in lists that correspond to training steps\n",
    "config[\"time_mask_max_size\"] = [0]\n",
    "config[\"time_mask_count\"] = [0]\n",
    "config[\"freq_mask_max_size\"] = [7]\n",
    "config[\"freq_mask_count\"] = [2]\n",
    "\n",
    "# Test the validation sets every this many steps\n",
    "config[\"eval_step_interval\"] = 500\n",
    "\n",
    "# Duration of the last layer before pooling or a fully connected layer\n",
    "config[\"clip_duration_ms\"] = 590  \n",
    "\n",
    "# The best model weights are chosen first by minimizing the specified minimization metric below the specified target_minimization\n",
    "# Once the target has been met, it chooses the maximum of the maximization metric. Set 'minimization_metric' to None to only maximize\n",
    "# Available metrics:\n",
    "#   - \"loss\" - cross entropy error on validation set\n",
    "#   - \"accuracy\" - accuracy of validation set\n",
    "#   - \"recall\" - recall of validation set\n",
    "#   - \"precision\" - precision of validation set\n",
    "#   - \"false_positive_rate\" - false positive rate of validation set\n",
    "#   - \"false_negative_rate\" - false negative rate of validation set\n",
    "#   - \"ambient_false_positives\" - count of false positives from the split validation_ambient set\n",
    "#   - \"ambient_false_positives_per_hour\" - estimated number of false positives per hour on the split validation_ambient set\n",
    "#   - \"average_viable_recall\" - the average recall rates for false accepts per hour rates between 0 and 2.0\n",
    "config[\"target_minimization\"] = 0.0\n",
    "\n",
    "config[\"minimization_metric\"] = None  # Set to None to disable and only maximize\n",
    "config[\"maximization_metric\"] = \"average_viable_recall\"\n",
    "\n",
    "with open(os.path.join(\"training_parameters.yaml\"), \"w\") as file:\n",
    "    documents = yaml.dump(config, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a MixedNet model. This produces great models and is very fast on the device while using small amounts of memory.\n",
    "!python -m microwakeword.model_train_eval \\\n",
    "--training_config='training_parameters.yaml' \\\n",
    "--train 1 \\\n",
    "--restore_checkpoint 1 \\\n",
    "--test_tf_nonstreaming 0 \\\n",
    "--test_tflite_nonstreaming 0 \\\n",
    "--test_tflite_streaming 1 \\\n",
    "--test_tflite_streaming_quantized 1 \\\n",
    "--use_weights \"best_weights\" \\\n",
    "mixednet \\\n",
    "--pointwise_filters \"48,48,48,48\" \\\n",
    "--repeat_in_block  \"1, 1, 1, 1\" \\\n",
    "--mixconv_kernel_sizes '[5], [9], [13], [21]' \\\n",
    "--residual_connection \"0,0,0,0\" \\\n",
    "--first_conv_filters 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an Inception model. It can produce good models, but it is slow at inference on the device and uses more memory, so you can't run multiple models at once. You may need to increase config[\"clip_duration_ms\"] so that the first layer's time dimension is at least 75\n",
    "!python -m microwakeword.model_train_eval \\\n",
    "--training_config='training_parameters.yaml' \\\n",
    "--train 0 \\\n",
    "--restore_checkpoint 1 \\\n",
    "--test_tf_nonstreaming 0 \\\n",
    "--test_tflite_nonstreaming 0 \\\n",
    "--test_tflite_streaming 0 \\\n",
    "--test_tflite_streaming_quantized 1 \\\n",
    "inception \\\n",
    "--cnn1_filters '32' \\\n",
    "--cnn1_kernel_sizes '5' \\\n",
    "--cnn1_subspectral_groups '1' \\\n",
    "--cnn2_filters1 '24,24,24' \\\n",
    "--cnn2_filters2 '32,64,96' \\\n",
    "--cnn2_kernel_sizes '3,5,5' \\\n",
    "--cnn2_subspectral_groups '1,1,1' \\\n",
    "--cnn2_dilation '1,1,1' \\\n",
    "--dropout 0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
