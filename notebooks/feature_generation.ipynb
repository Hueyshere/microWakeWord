{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads audio data for augmentation\n",
    "# Borrowed from openWakeWord's automatic_model_training.ipynb, accessed March 4, 2024\n",
    "\n",
    "import datasets\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Download MIR RIR data\n",
    "\n",
    "output_dir = \"./mit_rirs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    rir_dataset = datasets.load_dataset(\"davidscripka/MIT_environmental_impulse_responses\", split=\"train\", streaming=True)\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    for row in tqdm(rir_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1]\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "## Download noise and background audio\n",
    "\n",
    "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
    "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
    "# For full-scale training, it's recommended to download the entire dataset from\n",
    "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
    "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
    "\n",
    "if not os.path.exists(\"audioset\"):\n",
    "    os.mkdir(\"audioset\")\n",
    "\n",
    "    fname = \"bal_train09.tar\"\n",
    "    out_dir = f\"audioset/{fname}\"\n",
    "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
    "    !wget -O {out_dir} {link}\n",
    "    !cd audioset && tar -xvf bal_train09.tar\n",
    "\n",
    "    output_dir = \"./audioset_16k\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"audioset/audio\").glob(\"**/*.flac\")]})\n",
    "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "    for row in tqdm(audioset_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "# Free Music Archive dataset\n",
    "# https://github.com/mdeff/fma\n",
    "\n",
    "output_dir = \"./fma\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=True)\n",
    "    fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training\n",
    "    for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips\n",
    "        row = next(fma_dataset)\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "        i += 1\n",
    "        if i == n_hours*3600//30:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads already generated spectrogram features (made for microWakeWord in particular) for various negative datasets \n",
    "output_dir = './negative_datasets'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    link_root = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
    "    filenames = ['dinner_party_background.zip', 'no_speech_background.zip', 'speech_backround.zip']\n",
    "    for fname in filenames:\n",
    "        link = link_root + fname\n",
    "        \n",
    "        out_dir = f\"negative_datasets/{fname}\"\n",
    "        !wget -O {out_dir} {link}\n",
    "        !cd {output_dir} && unzip -q {fname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads mp3 clips in generated_samples/positive_validation and prepares to augment them\n",
    "\n",
    "from microwakeword.audio.augmentation import Augmentation\n",
    "from microwakeword.audio.clips import Clips\n",
    "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
    "\n",
    "clips = Clips(input_directory='generated_samples/positive/validation', \n",
    "              file_pattern='*.mp3', \n",
    "              max_clip_duration_s=None,\n",
    "              remove_silence=True, # HA Cloud TTS samples have extra silence at end, so trim it off first.\n",
    "              )\n",
    "augmenter = Augmentation(augmentation_duration_s=3.9,\n",
    "                         augmentation_probabilities = {\n",
    "                                \"SevenBandParametricEQ\": 0.25,\n",
    "                                \"TanhDistortion\": 0.25,\n",
    "                                \"PitchShift\": 0.25,\n",
    "                                \"BandStopFilter\": 0.25,\n",
    "                                \"AddColorNoise\": 0.25,\n",
    "                                \"AddBackgroundNoise\": 0.75,\n",
    "                                \"Gain\": 1.0,\n",
    "                                \"RIR\": 0.5,\n",
    "                            },\n",
    "                         impulse_paths = ['mit_rirs'],\n",
    "                         background_paths = ['fma', 'audioset'],\n",
    "                         background_min_snr_db = -10,\n",
    "                         background_max_snr_db = 0,\n",
    "                         min_jitter_s = 0.1,\n",
    "                         max_jitter_s = 0.2,\n",
    "                         )\n",
    "spectrograms = SpectrogramGeneration(clips=clips,\n",
    "                                     augmenter=augmenter,\n",
    "                                     slide_frames=5,    # Uses the same spectrogram 5 times, just shifted over by one frame. This simulates the streaming inferences while training/validating in nonstreaming mode.\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment a random clip and play it back to verify it works well\n",
    "import IPython\n",
    "from microwakeword.audio.audio_utils import save_clip\n",
    "\n",
    "random_clip = clips.get_random_clip()\n",
    "augmented_clip = augmenter.augment_clip(random_clip)\n",
    "save_clip(augmented_clip, 'augmented_clip.wav')\n",
    "\n",
    "IPython.display.display(IPython.display.Audio(\"augmented_clip.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented samples and save them for a validation set\n",
    "\n",
    "import os\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "\n",
    "if not os.path.exists('generated_sets'):\n",
    "    os.mkdir('generated_sets')\n",
    "if not os.path.exists('generated_sets/validation'):\n",
    "    os.mkdir('generated_sets/validation')\n",
    "\n",
    "RaggedMmap.from_generator(\n",
    "    out_dir='generated_sets/validation/wakeword_mmap',\n",
    "    sample_generator=spectrograms.spectrogram_generator(repeat=10),\n",
    "    batch_size=100,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate augmented spectorgrams for the testing set\n",
    "# Don't use slide_frames for testing set\n",
    "# You really should use a completely new set of samples from new voices rather than reuse the validation set...\n",
    "\n",
    "clips = Clips(input_directory='generated_samples/positive/validation', \n",
    "              file_pattern='*.mp3', \n",
    "              max_clip_duration_s=None,\n",
    "              remove_silence=True, # HA Cloud TTS samples have extra silence at end, so trim it off first.\n",
    "              )\n",
    "augmenter = Augmentation(augmentation_duration_s=3.9,\n",
    "                         augmentation_probabilities = {\n",
    "                                \"SevenBandParametricEQ\": 0.25,\n",
    "                                \"TanhDistortion\": 0.25,\n",
    "                                \"PitchShift\": 0.25,\n",
    "                                \"BandStopFilter\": 0.25,\n",
    "                                \"AddColorNoise\": 0.25,\n",
    "                                \"AddBackgroundNoise\": 0.75,\n",
    "                                \"Gain\": 1.0,\n",
    "                                \"RIR\": 0.5,\n",
    "                            },\n",
    "                         impulse_paths = ['mit_rirs'],\n",
    "                         background_paths = ['fma', 'audioset'],\n",
    "                         background_min_snr_db = -10,\n",
    "                         background_max_snr_db = 0,\n",
    "                         min_jitter_s = 0.1,\n",
    "                         max_jitter_s = 0.2,\n",
    "                         )\n",
    "spectrograms = SpectrogramGeneration(clips=clips,\n",
    "                                     augmenter=augmenter,\n",
    "                                     )\n",
    "RaggedMmap.from_generator(\n",
    "    out_dir='generated_sets/testing/wakeword_mmap',\n",
    "    sample_generator=spectrograms.spectrogram_generator(repeat=10),\n",
    "    batch_size=100,\n",
    "    verbose=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
